import numpy as np
import argparse
import json
import os
import random

import pandas as pd
from transformers import AutoTokenizer
from tqdm.auto import tqdm


from llama import call_pipe, clean_text_series, get_pipe, num_tokens, timeit

pd.options.display.width = 0

template = '<Item Title>: "{}"; <Item Description>: "{}"; <User Review> "{}"; <User Score>: {}'


def cut_to(sentence, tokenizer, max_length):
    return tokenizer.convert_tokens_to_string(tokenizer.tokenize(sentence)[: max_length])


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument('--model_name', type=str, default='meta-llama/Llama-2-7b-chat-hf')
    parser.add_argument('--data', '-d', type=str, default='data/sampled_Toys_and_Games')
    parser.add_argument('--temp', type=float, default=0.01)
    parser.add_argument(
        '--min_new_tokens',
        type=int,
        default=10,
        help='min number of new tokens generated by the model',
    )
    parser.add_argument(
        '--max_new_tokens',
        type=int,
        default=512,
        help='max number of new tokens generated by the model',
    )
    parser.add_argument(
        '--max_review_length',
        type=int,
        default=200,
        help='max length of a single review',
    )
    parser.add_argument(
        '--max_review_number',
        type=int,
        default=30,
        help='maximum number of reviews per user'
    )
    parser.add_argument('--batch_size', type=int, default=8)
    parser.add_argument('--seed', type=int, default=1)
    parser.add_argument('--prompt_path', type=str, default='prompts/prompts_proper.json')
    parser.add_argument('--regenerate', action='store_true', help='regenerate prompts')
    args = parser.parse_args()
    if 'Llama-2' in args.model_name:
        args.max_length = 4096
    elif 'Llama-3' in args.model_name:
        # args.max_length = 4096
        args.max_length = 8192
    else:
        raise ValueError(f'model not llama: {args.model_name}')
    return args


@timeit
def load_data(args):
    if os.path.exists(f'{args.data}/reviews_text_clean.tsv') and os.path.exists(f'{args.data}/meta_synced_clean.tsv'):
        reviews = pd.read_table(f'{args.data}/reviews_text_clean.tsv').dropna()
        meta = pd.read_table(f'{args.data}/meta_synced_clean.tsv').dropna()

    else:
        reviews = pd.read_table(f'{args.data}/reviews_text.tsv').dropna()
        meta = pd.read_table(f'{args.data}/meta_synced.tsv').dropna()
        tokenizer = AutoTokenizer.from_pretrained(args.model_name)
        reviews['review'] = clean_text_series(reviews.review).apply(cut_to, args=(tokenizer, args.max_review_length))
        meta['title'] = clean_text_series(meta.title)
        meta['description'] = clean_text_series(meta.description).apply(cut_to, args=(tokenizer, args.max_review_length))
        reviews.to_csv(f'{args.data}/reviews_text_clean.tsv', sep='\t', index=False)
        meta.to_csv(f'{args.data}/meta_synced_clean.tsv', sep='\t', index=False)

    train = pd.read_table(f'{args.data}/reshuffle_{args.seed}/train.tsv')
    test = pd.read_table(f'{args.data}/reshuffle_{args.seed}/test.tsv')

    users = set(train.user_id.unique()) | set(test.user_id.unique())

    reviews = reviews[reviews.user_id.isin(users)]

    train = train[train.user_id.isin(users)].groupby('user_id').head(args.max_review_number)  # only take max_review_number reviews
    test = test[test.user_id.isin(users)]

    train = train.merge(meta, on='asin')
    test = test.merge(meta, on='asin')
    train = train.merge(reviews, on=['user_id', 'asin', 'rating'])
    test = test.merge(reviews, on=['user_id', 'asin', 'rating'])

    return train, test, meta.set_index('asin')


def downsample(reviews, meta, n=10):
    vc = reviews.user_id.value_counts()
    reviews = reviews[(reviews.user_id.isin(vc[vc > 4].index)) & (reviews.user_id.isin(vc[vc < 10].index))]
    users = random.sample(list(reviews.user_id.unique()), n)
    reviews = reviews[reviews.user_id.isin(users)]
    meta = meta[meta.asin.isin(set(reviews.asin.unique()))]
    return reviews, meta


def process_group(group, prompt_len, args, prompt, tokenizer):
    filler = ''
    current_len = prompt_len
    for ind, row in enumerate(group.itertuples(), start=1):
        # control length of the final prompt by breaking early
        to_add = f'{ind}. ' + template.format(row.title, row.description, row.review, row.rating) + '\n'
        filler += to_add
        current_len += num_tokens(to_add, tokenizer)
        if current_len > args.max_length - args.max_new_tokens:
            break
    return prompt.format(filler)


@timeit
def generate_profile(pipe, data, args):
    with open(args.prompt_path, 'r') as file:
        prompt = pipe.tokenizer.apply_chat_template(
            json.load(file)['user_profile'],
            tokenize=False,
            add_generation_prompt=True,
        )
        prompt_len = num_tokens(prompt, pipe.tokenizer)

    if 'Llama-3' in args.model_name:
        profile_path = f'{args.data}/reshuffle_{args.seed}/profiles_{args.temp}_llama3.tsv'
    elif 'Llama-2' in args.model_name:
        profile_path = f'{args.data}/reshuffle_{args.seed}/profiles_{args.temp}_llama2.tsv'
    else:
        raise ValueError(f'model not llama: {args.model_name}')

    if os.path.exists(os.path.join(args.data, 'embeddings/profiles_cache.tsv')):
        cache = (
            pd.read_table(os.path.join(args.data, 'embeddings/profiles_cache.tsv'))
            .set_index('prompt_profile')['profile']
            .to_dict()
        )
    else:
        cache = {}

    if os.path.exists(profile_path) and not args.regenerate:
        descriptions = pd.read_table(profile_path)
    else:
        descriptions = []
        for user, group in tqdm(data.groupby('user_id'), dynamic_ncols=True):
            descriptions.append((user, process_group(group, prompt_len, args, prompt, tokenizer)))
        descriptions = pd.DataFrame(descriptions, columns=['user_id', 'prompt_profile']).set_index('user_id')
        descriptions.to_csv(profile_path, sep='\t')

    descriptions['profile'] = descriptions['prompt_profile'].map(lambda x: cache.get(x, np.nan))
    to_generate = descriptions[descriptions['profile'].isna()].prompt_profile.tolist()
    print('len(to_generate)', len(to_generate), 'instead of', len(descriptions))
    generated = call_pipe(
        pipe=pipe,
        queries=to_generate,
        batch_size=args.batch_size,
    )
    descriptions['profile'] = descriptions['profile'].astype(object)  # Cast 'profile' column to object dtype
    descriptions.loc[descriptions['profile'].isna(), 'profile'] = generated

    cache.update(descriptions.set_index('prompt_profile')['profile'].to_dict())
    pd.DataFrame(cache.items(), columns=['prompt_profile', 'profile']).to_csv(
        os.path.join(args.data, 'embeddings/profiles_cache.tsv'), sep='\t', index=False
    )

    descriptions = descriptions.drop('prompt_profile', axis=1)
    descriptions.to_csv(profile_path, sep='\t')
    return descriptions


@timeit
def rate_items(pipe, profiles, data, args):
    with open(args.prompt_path, 'r') as file:
        prompt = pipe.tokenizer.apply_chat_template(json.load(file)['rating'], tokenize=False, add_generation_prompt=True)

    if 'Llama-3' in args.model_name:
        rating_path = f'rating_responses_{args.temp}_llama3.tsv'
    elif 'Llama-2' in args.model_name:
        rating_path = f'rating_responses_{args.temp}_llama2.tsv'
    else:
        raise ValueError(f'model not llama: {args.model_name}')

    data['profile'] = data['user_id'].map(profiles['profile'])
    data['prompt_inference'] = prompt
    data['prompt_inference'] = data.apply(lambda x: x['prompt_inference'].format(x['profile'], x['title'], x['description']), axis=1)
    data.to_csv(rating_path, sep='\t', index=False)
    data['response'] = call_pipe(pipe=pipe, queries=data['prompt_inference'].tolist(), batch_size=args.batch_size)
    data['pred_score'] = data['response'].apply(lambda x: x.split("Final score:")[1] if 'Final score:' in x else np.nan)
    data.drop('prompt_inference', axis=1).to_csv(rating_path, sep='\t', index=False)


def rank_items(pipe, profiles, meta, args):
    with open(args.prompt_path, 'r') as file:
        prompt = pipe.tokenizer.apply_chat_template(json.load(file)['ranking'], tokenize=False, add_generation_prompt=True)

    ranking_path = 'reranking_responses.tsv'

    model = load_lightgcn(args)
    prediction = model.predict(users=profiles.index, top_n=50)
    profiles['y_pred'] = [[model.item_mapping_dict[i] for i in row] for row in prediction]

    def compose_rerank_prompt(row):
        items = []
        for ind, item in enumerate(row['y_pred'], start=1):
            items.append(f'{ind}. Title: \"{meta.loc[item, "title"]}\". Description: \"{meta.loc[item, "description"]}\"')
        return prompt.format(profiles.loc[row.name, 'profile'], '\n'.join(items))

    profiles['rerank_prompt'] = profiles.apply(compose_rerank_prompt, axis=1)
    profiles.to_csv(ranking_path, sep='\t', index=False)
    print('rerank prompts lens:', profiles['rerank_prompt'].apply(num_tokens, args=(pipe.tokenizer,)).tolist(), flush=True)

    profiles['rerank_response'] = call_pipe(pipe=pipe, queries=profiles['rerank_prompt'].tolist(), batch_size=args.batch_size)
    profiles = profiles.drop('rerank_prompt', axis=1)
    profiles.to_csv(ranking_path, sep='\t', index=False)


def main():
    args = parse_args()
    random.seed(args.seed)
    pipe = get_pipe(args, quantization='bfloat16')
    train, test, meta = load_data(args)

    profiles = generate_profile(pipe, train, args)

    # rate_items(pipe, profiles, test, args)
    # rank_items(pipe, profiles, meta, args)
    # infer_scores(pipe, test, profiles, args)


if __name__ == '__main__':
    main()
